{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35730a1b-406f-4cec-ba79-2603c2de68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "os.environ[\"GDAL_DATA\"] = \"/home/parndt/anaconda3/envs/geo_py37/share/gdal\"\n",
    "os.environ[\"PROJ_LIB\"] = \"/Users/parndt/anaconda3/envs/geo_py37/share/proj\"\n",
    "os.environ[\"PROJ_DATA\"] = \"/Users/parndt/anaconda3/envs/geo_py37/share/proj\"\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import h5py\n",
    "import math\n",
    "import shutil\n",
    "import zipfile\n",
    "import shapely\n",
    "import requests\n",
    "import datetime\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry.polygon import orient\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from xml.etree import ElementTree as ET\n",
    "import hdbscan\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from cmcrameri import cm as cmc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import Image, display\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.signal import find_peaks\n",
    "import rasterio as rio\n",
    "from rasterio import plot as rioplot\n",
    "from rasterio import warp\n",
    "from rasterio import Affine as A\n",
    "from rasterio.enums import ColorInterp\n",
    "from rasterio.windows import Window\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import TransformMethodsMixin\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import reproject\n",
    "from rasterio.crs import CRS\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from scipy import signal\n",
    "from utils import read_atl03\n",
    "from utils import intersection\n",
    "from ed.edcreds import getedcreds\n",
    "\n",
    "def download_is2(short_name='ATL03', start_date='2018-01-01', end_date='2030-01-01', uid='userid', pwd='pwd', \n",
    "                 bbox=None, shape=None, vars_sub='all', output_dir='nsidc_outputs'):\n",
    "    \n",
    "    bounding_box = '%.7f,%.7f,%.7f,%.7f' % tuple(boundbox)\n",
    "\n",
    "    start_time = '00:00:00'\n",
    "    end_time = '23:59:59'\n",
    "    temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "\n",
    "    cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "    granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "    base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "    # Get json response from CMR collection metadata\n",
    "    params = {'short_name': short_name}\n",
    "    response = requests.get(cmr_collections_url, params=params)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    # Find all instances of 'version_id' in metadata and print most recent version number\n",
    "    versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "    latest_version = max(versions)\n",
    "    capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "    search_params = {'short_name': short_name, 'version': latest_version, 'temporal': temporal, 'page_size': 100, 'page_num': 1}\n",
    "    if boundbox is not None: search_params['bounding_box'] = bounding_box\n",
    "    elif shape is not None: search_params['polygon'] = polygon\n",
    "    else: print('No spatial filtering criteria were given.')\n",
    "\n",
    "    headers= {'Accept': 'application/json'}\n",
    "    print(\"Search parameters:\", search_params)\n",
    "\n",
    "    # query for granules \n",
    "    granules = []\n",
    "    headers={'Accept': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "        results = json.loads(response.content)\n",
    "        if len(results['feed']['entry']) == 0: break\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        search_params['page_num'] += 1\n",
    "\n",
    "    print('\\nFound %i %s version %s granules over the search area between %s and %s.' % (len(granules), short_name, latest_version, \n",
    "                                                                              start_date, end_date))\n",
    "    if len(granules) == 0: print('None')\n",
    "    for result in granules:\n",
    "        print('  '+result['producer_granule_id'], f', {float(result[\"granule_size\"]):.2f} MB',sep='')\n",
    "\n",
    "    if shape is not None:\n",
    "        gdf = gpd.read_file(geojson_filepath)\n",
    "\n",
    "        # Simplify polygon for complex shapes in order to pass a reasonable request length to CMR. \n",
    "        # The larger the tolerance value, the more simplified the polygon.\n",
    "        # Orient counter-clockwise: CMR polygon points need to be provided in counter-clockwise order. \n",
    "        # The last point should match the first point to close the polygon.\n",
    "        poly = orient(gdf.simplify(0.05, preserve_topology=False).loc[0],sign=1.0)\n",
    "\n",
    "        geojson_data = gpd.GeoSeries(poly).to_json() # Convert to geojson\n",
    "        geojson_data = geojson_data.replace(' ', '') #remove spaces for API call\n",
    "\n",
    "        #Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "        polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "\n",
    "        print('\\nInput geojson:', geojson)\n",
    "        print('Simplified polygon coordinates based on geojson input:', polygon)\n",
    "\n",
    "    # Create session to store cookie and pass credentials to capabilities url\n",
    "    session = requests.session()\n",
    "    s = session.get(capability_url)\n",
    "    response = session.get(s.url,auth=(uid,pwd))\n",
    "\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    #collect lists with each service option\n",
    "    subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "    # this is for getting possible variable values from the granule search\n",
    "    if len(subagent) > 0 :\n",
    "        # variable subsetting\n",
    "        variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "        variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "        variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "        variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "    # make sure to only request the variables that are available\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "    if vars_sub == 'all':\n",
    "        var_list_subsetting = ''\n",
    "    else:\n",
    "        var_list_subsetting = intersection(variable_vals,var_list)\n",
    "\n",
    "    if len(subagent) < 1 :\n",
    "        print('No services exist for', short_name, 'version', latest_version)\n",
    "        agent = 'NO'\n",
    "        coverage,Boundingshape = '',''\n",
    "    else:\n",
    "        agent = ''\n",
    "        subdict = subagent[0]\n",
    "        if subdict['spatialSubsettingShapefile'] == 'true':\n",
    "            if boundbox is not None:\n",
    "                Boundingshape, polygon, bbox = '', '', bounding_box\n",
    "            if shape is not None:\n",
    "                Boundingshape, bbox = '', geojson_data\n",
    "            else:\n",
    "                Boundingshape = ''\n",
    "        coverage = ','.join(var_list_subsetting)\n",
    "\n",
    "    #Set the request mode to asynchronous if the number of granules is over 100, otherwise synchronous is enabled by default\n",
    "    if len(granules) > 100:\n",
    "        request_mode = 'async'\n",
    "        page_size = 2000\n",
    "    else: \n",
    "        page_size = 100\n",
    "        request_mode = 'stream'\n",
    "    #Determine number of orders needed for requests over 2000 granules. \n",
    "    page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "    print('  --> There will be', page_num, 'total order(s) processed for our', short_name, 'request.')\n",
    "    param_dict = {'short_name': short_name, \n",
    "                  'version': latest_version, \n",
    "                  'temporal': temporal, \n",
    "                  'bbox': bbox,\n",
    "                  'bounding_box': bounding_box,\n",
    "                  'Boundingshape': Boundingshape, \n",
    "                  'polygon': polygon,\n",
    "                  'Coverage': coverage, \n",
    "                  'page_size': page_size, \n",
    "                  'request_mode': request_mode, \n",
    "                  'agent': agent, \n",
    "                  'email': email, }\n",
    "\n",
    "    #Remove blank key-value-pairs\n",
    "    param_dict = {k: v for k, v in param_dict.items() if v != ''}\n",
    "\n",
    "    #Convert to string\n",
    "    param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items())\n",
    "    param_string = param_string.replace(\"'\",\"\")\n",
    "\n",
    "    #Print API base URL + request parameters\n",
    "    endpoint_list = [] \n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        API_request = api_request = f'{base_url}?{param_string}&page_num={page_val}'\n",
    "        endpoint_list.append(API_request)\n",
    "\n",
    "    print('\\n', *endpoint_list, sep = \"\\n\") \n",
    "\n",
    "    # Create an output folder if the folder does not already exist.\n",
    "    path = str(os.getcwd() + '/' + output_dir)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    # Different access methods depending on request mode:\n",
    "    if request_mode=='async':\n",
    "        # Request data service for each page number, and unzip outputs\n",
    "        for i in range(page_num):\n",
    "            page_val = i + 1\n",
    "            print('Order: ', page_val)\n",
    "\n",
    "        # For all requests other than spatial file upload, use get function\n",
    "            param_dict['page_num'] = page_val\n",
    "            request = session.get(base_url, params=param_dict)\n",
    "\n",
    "            print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "            request.raise_for_status()\n",
    "            print('Order request URL: ', request.url)\n",
    "            esir_root = ET.fromstring(request.content)\n",
    "            print('Order request response XML content: ', request.content)\n",
    "\n",
    "        #Look up order ID\n",
    "            orderlist = []   \n",
    "            for order in esir_root.findall(\"./order/\"):\n",
    "                orderlist.append(order.text)\n",
    "            orderID = orderlist[0]\n",
    "            print('order ID: ', orderID)\n",
    "\n",
    "        #Create status URL\n",
    "            statusURL = base_url + '/' + orderID\n",
    "            print('status URL: ', statusURL)\n",
    "\n",
    "        #Find order status\n",
    "            request_response = session.get(statusURL)    \n",
    "            print('HTTP response from order response URL: ', request_response.status_code)\n",
    "\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "            request_response.raise_for_status()\n",
    "            request_root = ET.fromstring(request_response.content)\n",
    "            statuslist = []\n",
    "            for status in request_root.findall(\"./requestStatus/\"):\n",
    "                statuslist.append(status.text)\n",
    "            status = statuslist[0]\n",
    "            print('Data request ', page_val, ' is submitting...')\n",
    "            print('Initial request status is ', status)\n",
    "\n",
    "        #Continue loop while request is still processing\n",
    "            while status == 'pending' or status == 'processing': \n",
    "                print('Status is not complete. Trying again.')\n",
    "                time.sleep(10)\n",
    "                loop_response = session.get(statusURL)\n",
    "\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "                loop_response.raise_for_status()\n",
    "                loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "        #find status\n",
    "                statuslist = []\n",
    "                for status in loop_root.findall(\"./requestStatus/\"):\n",
    "                    statuslist.append(status.text)\n",
    "                status = statuslist[0]\n",
    "                print('Retry request status is: ', status)\n",
    "                if status == 'pending' or status == 'processing':\n",
    "                    continue\n",
    "\n",
    "        #Order can either complete, complete_with_errors, or fail:\n",
    "        # Provide complete_with_errors error message:\n",
    "            if status == 'complete_with_errors' or status == 'failed':\n",
    "                messagelist = []\n",
    "                for message in loop_root.findall(\"./processInfo/\"):\n",
    "                    messagelist.append(message.text)\n",
    "                print('error messages:')\n",
    "                pprint.pprint(messagelist)\n",
    "\n",
    "        # Download zipped order if status is complete or complete_with_errors\n",
    "            if status == 'complete' or status == 'complete_with_errors':\n",
    "                downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "                print('Zip download URL: ', downloadURL)\n",
    "                print('Beginning download of zipped output...')\n",
    "                zip_response = session.get(downloadURL)\n",
    "                # Raise bad request: Loop will stop for bad response code.\n",
    "                zip_response.raise_for_status()\n",
    "                with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "                    z.extractall(path)\n",
    "                print('Data request', page_val, 'is complete.')\n",
    "            else: print('Request failed.')\n",
    "\n",
    "    else:\n",
    "        for i in range(page_num):\n",
    "            page_val = i + 1\n",
    "            print('\\nOrder: ', page_val)\n",
    "            print('Requesting...')\n",
    "            request = session.get(base_url, params=param_dict)\n",
    "            print('HTTP response from order response URL: ', request.status_code)\n",
    "            request.raise_for_status()\n",
    "            d = request.headers['content-disposition']\n",
    "            fname = re.findall('filename=(.+)', d)\n",
    "            dirname = os.path.join(path,fname[0].strip('\\\"'))\n",
    "            print('Downloading...')\n",
    "            open(dirname, 'wb').write(request.content)\n",
    "            print('Data request', page_val, 'is complete.')\n",
    "\n",
    "        # Unzip outputs\n",
    "        for z in os.listdir(path): \n",
    "            if z.endswith('.zip'): \n",
    "                zip_name = path + \"/\" + z \n",
    "                zip_ref = zipfile.ZipFile(zip_name) \n",
    "                zip_ref.extractall(path) \n",
    "                zip_ref.close() \n",
    "                os.remove(zip_name)\n",
    "\n",
    "    # Clean up Outputs folder by removing individual granule folders \n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "            try:\n",
    "                shutil.move(os.path.join(root, file), path)\n",
    "            except OSError:\n",
    "                pass\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83372800-0841-469f-88c0-99c6c12815f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search parameters: {'short_name': 'ATL03', 'version': '005', 'temporal': '2021-09-27T00:00:00Z,2021-09-27T23:59:59Z', 'page_size': 100, 'page_num': 1, 'bounding_box': '-116.7732992,34.0385789,-116.7189984,34.0991181'}\n",
      "\n",
      "Found 2 ATL03 version 005 granules over the search area between 2021-09-27 and 2021-09-27.\n",
      "  ATL03_20210927194224_00821302_005_01.h5, 5508.05 MB\n",
      "  ATL03_20210927194224_00821302_005_01.h5, 5508.05 MB\n",
      "  --> There will be 1 total order(s) processed for our ATL03 request.\n",
      "\n",
      "\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL03&version=005&temporal=2021-09-27T00:00:00Z,2021-09-27T23:59:59Z&bbox=-116.7732992,34.0385789,-116.7189984,34.0991181&bounding_box=-116.7732992,34.0385789,-116.7189984,34.0991181&page_size=100&request_mode=stream&email=philipp.arndt@aya.yale.edu&page_num=1\n",
      "\n",
      "Order:  1\n",
      "Requesting...\n",
      "HTTP response from order response URL:  200\n",
      "Downloading...\n",
      "Data request 1 is complete.\n"
     ]
    }
   ],
   "source": [
    "# for San G Veg offpointing track 82 2021-09-27\n",
    "dtm_bbox = [[520925.877649,525919.746883],[3766456.711576,3773181.723345]]\n",
    "coords_latlon = warp.transform({'init': 'epsg:32611'}, {'init': 'epsg:4326'}, dtm_bbox[0], dtm_bbox[1])\n",
    "boundbox = list(np.array(coords_latlon).T.flatten())\n",
    "start_date = '2021-09-27'\n",
    "end_date = '2021-09-27'\n",
    "output_dir = 'data/IS2/sang_test_veg_offpointing'\n",
    "uid, pwd, email = getedcreds()\n",
    "download_is2(start_date=start_date, end_date=end_date, uid=uid, pwd=pwd, bbox=boundbox, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e787b997-59d6-4d20-aa7d-e02d07b64859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([520925.8776490004, 525919.7468830011],\n",
       " [3766456.7115759994, 3773181.7233450003])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_dtm = warp.transform({'init': 'epsg:4326'},{'init': 'epsg:32611'}, [boundbox[0], boundbox[2]],[boundbox[1],boundbox[3]])\n",
    "coords_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa2f76-35cb-4bb5-9fdc-1b6d9f8396ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_py37",
   "language": "python",
   "name": "geo_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
